import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import MultinomialNB
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import (
	accuracy_score,
	precision_score,
	recall_score,
	f1_score,
	classification_report,
	confusion_matrix
)
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time

# Create directories
os.makedirs('models', exist_ok=True)
os.makedirs('docs', exist_ok=True)

print("="*70)
print("SECUECODE AI - MODEL TRAINING")
print("="*70)

# Load dataset
print("\nüìÇ Loading dataset...")
df = pd.read_csv('data/balanced_merged_dataset.csv')
print(f"‚úì Total samples: {len(df):,}")
print(f"  Vulnerable: {sum(df['is_vulnerable'] == 1):,}")
print(f"  Safe: {sum(df['is_vulnerable'] == 0):,}")

# Show dataset sources if available
if 'source' in df.columns:
	print("\nüìä Dataset sources:")
	for source in df['source'].unique():
		count = sum(df['source'] == source)
		vuln = sum((df['source'] == source) & (df['is_vulnerable'] == 1))
		safe = count - vuln
		print(f"  {source}: {count:,} samples ({vuln:,} vuln, {safe:,} safe)")

# Prepare data
X = df['code']
y = df['is_vulnerable']

# Split data
print("\n‚úÇÔ∏è  Splitting data...")
X_train, X_test, y_train, y_test = train_test_split(
	X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"  Training samples: {len(X_train):,}")
print(f"  Testing samples: {len(X_test):,}")

# Vectorize text
print("\nüî§ Vectorizing code samples...")
vectorizer = TfidfVectorizer(
	max_features=3000,      # Reduced for faster training
	ngram_range=(1, 2),
	min_df=2,
	max_df=0.95,
	sublinear_tf=True
)

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

print(f"‚úì Vectorization complete")
print(f"  Feature dimensions: {X_train_vec.shape[1]:,}")

# Save vectorizer
joblib.dump(vectorizer, 'models/vectorizer.joblib')
print("‚úì Vectorizer saved to models/vectorizer.joblib")

# Define all 6 models
print("\n" + "="*70)
print("DEFINING MODELS")
print("="*70)

models = {
	'Logistic Regression': {
		'model': LogisticRegression(
			max_iter=1000,
			random_state=42,
			n_jobs=-1,
			verbose=0
		),
		'description': 'Linear classifier with L2 regularization'
	},

	'Random Forest': {
		'model': RandomForestClassifier(
			n_estimators=100,
			max_depth=20,
			min_samples_split=5,
			min_samples_leaf=2,
			random_state=42,
			n_jobs=-1,
			verbose=0
		),
		'description': 'Ensemble of 100 decision trees'
	},

	'Gradient Boosting': {
		'model': GradientBoostingClassifier(
			n_estimators=100,
			learning_rate=0.1,
			max_depth=5,
			subsample=0.8,
			min_samples_split=5,
			min_samples_leaf=2,
			random_state=42,
			verbose=0
		),
		'description': 'Sequential boosting with decision trees'
	},

	'XGBoost': {
		'model': xgb.XGBClassifier(
			n_estimators=100,
			max_depth=6,
			learning_rate=0.1,
			subsample=0.8,
			colsample_bytree=0.8,
			gamma=0,
			min_child_weight=1,
			random_state=42,
			n_jobs=-1,
			eval_metric='logloss',
			verbosity=0
		),
		'description': 'Optimized gradient boosting (XGBoost)'
	},

	'LightGBM': {
		'model': lgb.LGBMClassifier(
			n_estimators=100,
			max_depth=6,
			learning_rate=0.1,
			num_leaves=31,
			subsample=0.8,
			colsample_bytree=0.8,
			min_child_samples=20,
			random_state=42,
			n_jobs=-1,
			verbose=-1
		),
		'description': 'Fast gradient boosting (LightGBM)'
	},

	'Naive Bayes': {
		'model': MultinomialNB(
			alpha=1.0,
			fit_prior=True
		),
		'description': 'Probabilistic classifier with Laplace smoothing'
	}
}

print(f"\n‚úì {len(models)} models defined:")
for name, info in models.items():
	print(f"  ‚Ä¢ {name}: {info['description']}")

# Store results
results = {}
training_times = {}

# Train and evaluate each model
print("\n" + "="*70)
print("TRAINING MODELS")
print("="*70)

for model_name, model_info in models.items():
	print(f"\n{'='*70}")
	print(f"ü§ñ Training: {model_name}")
	print(f"{'='*70}")
	print(f"Description: {model_info['description']}")

	model = model_info['model']

	# Train model with timing
	print(f"\n‚è±Ô∏è  Training started...")
	start_time = time.time()

	model.fit(X_train_vec, y_train)

	training_time = time.time() - start_time
	training_times[model_name] = training_time

	print(f"‚úì Training completed in {training_time:.2f} seconds")

	# Make predictions
	print(f"üîç Evaluating on test set...")
	y_pred = model.predict(X_test_vec)
	y_pred_proba = model.predict_proba(X_test_vec)

	# Calculate metrics
	accuracy = accuracy_score(y_test, y_pred)
	precision = precision_score(y_test, y_pred)
	recall = recall_score(y_test, y_pred)
	f1 = f1_score(y_test, y_pred)

	# Store results
	results[model_name] = {
		'accuracy': accuracy,
		'precision': precision,
		'recall': recall,
		'f1_score': f1,
		'training_time': training_time,
		'predictions': y_pred,
		'probabilities': y_pred_proba
	}

	# Print results
	print(f"\nüìä Results:")
	print(f"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
	print(f"  Precision: {precision:.4f}")
	print(f"  Recall:    {recall:.4f}")
	print(f"  F1-Score:  {f1:.4f}")

	print(f"\n{'-'*70}")
	print("Classification Report:")
	print(f"{'-'*70}")
	print(classification_report(y_test, y_pred,
		  target_names=['Safe', 'Vulnerable'], digits=4))

	# Confusion Matrix
	cm = confusion_matrix(y_test, y_pred)
	print(f"\n{'-'*70}")
	print("Confusion Matrix:")
	print(f"{'-'*70}")
	print(f"                 Predicted")
	print(f"                Safe  Vulnerable")
	print(f"Actual Safe     {cm[0][0]:>5}  {cm[0][1]:>5}")
	print(f"       Vuln     {cm[1][0]:>5}  {cm[1][1]:>5}")

	# Plot confusion matrix
	plt.figure(figsize=(8, 6))
	sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
				xticklabels=['Safe', 'Vulnerable'],
				yticklabels=['Safe', 'Vulnerable'],
				cbar_kws={'label': 'Count'})
	plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')
	plt.ylabel('Actual Label', fontsize=12)
	plt.xlabel('Predicted Label', fontsize=12)

	# Save plot
	filename = model_name.lower().replace(' ', '_')
	plt.savefig(f'docs/confusion_matrix_{filename}.png', dpi=300, bbox_inches='tight')
	plt.close()
	print(f"‚úì Confusion matrix saved to docs/confusion_matrix_{filename}.png")

	# Save model
	model_filename = f'models/model_{filename}.joblib'
	joblib.dump(model, model_filename)
	print(f"‚úì Model saved to {model_filename}")

# Compare all models
print("\n" + "="*70)
print("MODEL COMPARISON")
print("="*70)

comparison_df = pd.DataFrame({
	'Model': list(results.keys()),
	'Accuracy': [r['accuracy'] for r in results.values()],
	'Precision': [r['precision'] for r in results.values()],
	'Recall': [r['recall'] for r in results.values()],
	'F1-Score': [r['f1_score'] for r in results.values()],
	'Training Time (s)': [r['training_time'] for r in results.values()]
})

# Sort by accuracy
comparison_df = comparison_df.sort_values('Accuracy', ascending=False)

print("\n" + comparison_df.to_string(index=False))

# Find best models
best_accuracy_model = comparison_df.iloc[0]['Model']
best_accuracy = comparison_df.iloc[0]['Accuracy']
fastest_model = comparison_df.loc[comparison_df['Training Time (s)'].idxmin(), 'Model']
fastest_time = comparison_df['Training Time (s)'].min()

print(f"\nüèÜ Best Accuracy: {best_accuracy_model} ({best_accuracy:.4f})")
print(f"‚ö° Fastest Training: {fastest_model} ({fastest_time:.2f}s)")

# Save comparison results
comparison_df.to_csv('docs/model_comparison.csv', index=False)
print(f"‚úì Comparison results saved to docs/model_comparison.csv")

# Summary statistics
print("\n" + "="*70)
print("SUMMARY STATISTICS")
print("="*70)

print(f"\nüìà Performance Range:")
print(f"  Accuracy:  {comparison_df['Accuracy'].min():.4f} - {comparison_df['Accuracy'].max():.4f}")
print(f"  Precision: {comparison_df['Precision'].min():.4f} - {comparison_df['Precision'].max():.4f}")
print(f"  Recall:    {comparison_df['Recall'].min():.4f} - {comparison_df['Recall'].max():.4f}")
print(f"  F1-Score:  {comparison_df['F1-Score'].min():.4f} - {comparison_df['F1-Score'].max():.4f}")

print(f"\n‚è±Ô∏è  Training Time Range:")
print(f"  Fastest: {comparison_df['Training Time (s)'].min():.2f}s")
print(f"  Slowest: {comparison_df['Training Time (s)'].max():.2f}s")
print(f"  Average: {comparison_df['Training Time (s)'].mean():.2f}s")
print(f"  Total:   {comparison_df['Training Time (s)'].sum():.2f}s")

print("\n" + "="*70)
print("‚úÖ TRAINING COMPLETE!")
print("="*70)
print(f"\nüìÅ Models saved in 'models/' directory:")
for name in results.keys():
	filename = name.lower().replace(' ', '_')
	print(f"  ‚úì model_{filename}.joblib")
print(f"  ‚úì vectorizer.joblib")

print(f"\nComparison results saved in 'docs/' directory:")
print(f"  ‚úì model_comparison.csv")
